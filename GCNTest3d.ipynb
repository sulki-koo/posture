{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783eb4be",
   "metadata": {},
   "source": [
    "# 공통 모델 등 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afbc0421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- GCN 모델 클래스 ---\n",
    "class GCNNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, hidden_channels=128, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        out = self.lin(x)\n",
    "        return out\n",
    "\n",
    "# --- MediaPipe 스켈레톤 엣지 (33개 노드) ---\n",
    "edge_index = torch.tensor([\n",
    "    [0,1, 1,2, 2,3, 3,7, 7,9, 9,11, 11,13, 13,15,\n",
    "     2,4, 4,6, 6,8, 8,10, 10,12, 12,14, 14,16,\n",
    "     2,17, 17,19, 19,21, 21,23, 23,25, 25,27,\n",
    "     2,18, 18,20, 20,22, 22,24, 24,26, 26,28],\n",
    "    [1,0, 2,1, 3,2, 7,3, 9,7, 11,9, 13,11, 15,13,\n",
    "     4,2, 6,4, 8,6, 10,8, 12,10, 14,12, 16,14,\n",
    "     17,2, 19,17, 21,19, 23,21, 25,23, 27,25,\n",
    "     18,2, 20,18, 22,20, 24,22, 26,24, 28,26]\n",
    "], dtype=torch.long)\n",
    "\n",
    "edge_index_vis = torch.tensor([\n",
    "    [1,2], [2,3], [3,7], [4,5], [5,6], [6,8], # 얼굴\n",
    "    [1,9], [11,13], [13,15], [15,17], [15,19], [15,21], # 왼팔\n",
    "    [4,10], [12,14], [14,16], [16,18], [16,20], [16,22], # 오른팔\n",
    "    [23,25], [25,27], [27,29], [27,31], [29,31], # 왼다리\n",
    "    [24,26], [26,28], [28,30], [28,32], [30,32], # 오른다리\n",
    "    [11,12], [23,24], [11,23], [12,24]\n",
    "], dtype=torch.long).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3715de69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1754197651.037724  272493 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1754197651.187561  319665 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1754197651.241055  319669 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# --- 관절 완전성 체크 ---\n",
    "VISIBILITY_THRESHOLD = 0.6\n",
    "LEFT_LINE = [11, 13, 15, 23, 25, 27]\n",
    "RIGHT_LINE = [12, 14, 16, 24, 26, 28]\n",
    "\n",
    "def is_full_body(landmarks):\n",
    "    def check_line(line):\n",
    "        return all(landmarks[i].visibility >= VISIBILITY_THRESHOLD for i in line)\n",
    "    return check_line(LEFT_LINE) or check_line(RIGHT_LINE)\n",
    "\n",
    "def draw_skeleton(image, landmarks, edges):\n",
    "    h, w, _ = image.shape\n",
    "    for edge in edges.t().tolist():\n",
    "        start_idx, end_idx = edge\n",
    "        start = landmarks[start_idx]\n",
    "        end = landmarks[end_idx]\n",
    "        x1, y1 = int(start.x * w), int(start.y * h)\n",
    "        x2, y2 = int(end.x * w), int(end.y * h)\n",
    "        cv2.line(image, (x1, y1), (x2, y2), (255, 255, 0), 2)\n",
    "\n",
    "# --- 모델 및 레이블 인코더 로드 ---\n",
    "model_path = \"./best_gcn_model.pth\"\n",
    "label_encoder_path = \"./label_encoder.pkl\"\n",
    "\n",
    "model = GCNNet()  # 반드시 정의/임포트 필요\n",
    "model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "model.eval()\n",
    "\n",
    "classes = np.load(label_encoder_path, allow_pickle=True)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.classes_ = classes\n",
    "\n",
    "mp_pose = mp.solutions.pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=2,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e984129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 각도 계산 함수 (z 이진값 가중치 반영, use_z 플래그 포함) ---\n",
    "def calculate_angle_weighted(a, b, c, z_binary_a=1, z_binary_b=1, z_binary_c=1, use_z=True):\n",
    "    if not use_z:\n",
    "        # 2D 벡터 (Z축 무시)\n",
    "        ba = np.array([a.x - b.x, a.y - b.y])\n",
    "        bc = np.array([c.x - b.x, c.y - b.y])\n",
    "    else:\n",
    "        # 3D 벡터 (Z축 포함)\n",
    "        ba = np.array([\n",
    "            a.x - b.x,\n",
    "            a.y - b.y,\n",
    "            (a.z - b.z) * ((z_binary_a + z_binary_b) / 2)\n",
    "        ])\n",
    "        bc = np.array([\n",
    "            c.x - b.x,\n",
    "            c.y - b.y,\n",
    "            (c.z - b.z) * ((z_binary_c + z_binary_b) / 2)\n",
    "        ])\n",
    "\n",
    "    norm_ba = np.linalg.norm(ba)\n",
    "    norm_bc = np.linalg.norm(bc)\n",
    "    if norm_ba < 1e-6 or norm_bc < 1e-6:\n",
    "        return 0.0\n",
    "\n",
    "    cosine_angle = np.dot(ba, bc) / (norm_ba * norm_bc)\n",
    "    return np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))\n",
    "\n",
    "def calculate_neck_tilt_weighted(landmarks, z_binary, use_z):\n",
    "    ear = landmarks[7] if landmarks[7].visibility > landmarks[8].visibility else landmarks[8]\n",
    "    shoulder = landmarks[11] if landmarks[11].visibility > landmarks[12].visibility else landmarks[12]\n",
    "    hip = landmarks[23] if landmarks[23].visibility > landmarks[24].visibility else landmarks[24]\n",
    "    return calculate_angle_weighted(\n",
    "        ear, shoulder, hip,\n",
    "        z_binary[7], z_binary[11], z_binary[23],\n",
    "        use_z\n",
    "    )\n",
    "\n",
    "def calculate_waist_angle_weighted(landmarks, z_binary, use_z):\n",
    "    shoulder = landmarks[11] if landmarks[11].visibility > landmarks[12].visibility else landmarks[12]\n",
    "    hip = landmarks[23] if landmarks[23].visibility > landmarks[24].visibility else landmarks[24]\n",
    "    knee = landmarks[25] if landmarks[25].visibility > landmarks[26].visibility else landmarks[26]\n",
    "    return calculate_angle_weighted(\n",
    "        shoulder, hip, knee,\n",
    "        z_binary[11], z_binary[23], z_binary[25],\n",
    "        use_z\n",
    "    )\n",
    "\n",
    "def binary_z_values(landmarks, threshold=0.0):\n",
    "    return [1 if lm.z > threshold else 0 for lm in landmarks]\n",
    "\n",
    "POSTURE_ANGLE_THRESHOLDS = {\n",
    "    'stand': {\n",
    "        'front': {'neck_min': 165, 'neck_max': 180, 'waist_min': 165, 'waist_max': 180},\n",
    "        'side':  {'neck_min': 170, 'neck_max': 180, 'waist_min': 170, 'waist_max': 180}\n",
    "    },\n",
    "    'sit': {\n",
    "        'front': {'neck_min': 165, 'neck_max': 180, 'waist_min': 100, 'waist_max': 125},\n",
    "        'side':  {'neck_min': 170, 'neck_max': 180, 'waist_min': 100, 'waist_max': 110}\n",
    "    }\n",
    "}\n",
    "\n",
    "def check_posture_angles(posture, neck, waist, direction):\n",
    "    t = POSTURE_ANGLE_THRESHOLDS.get(posture, {}).get(direction)\n",
    "    if not t:\n",
    "        return False\n",
    "    return (\n",
    "        t['neck_min'] <= neck <= t['neck_max'] and\n",
    "        t['waist_min'] <= waist <= t['waist_max']\n",
    "    )\n",
    "\n",
    "# --- 프레임 처리 함수 ---\n",
    "def process_frame(image, model, label_encoder, edge_index, mp_pose, threshold_z=0.0):\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = mp_pose.process(image_rgb)\n",
    "    if not results.pose_landmarks:\n",
    "        return None, image\n",
    "\n",
    "    landmarks = results.pose_landmarks.landmark\n",
    "    z_binary = binary_z_values(landmarks, threshold=threshold_z)\n",
    "\n",
    "    left_pattern = [z_binary[i] for i in LEFT_LINE]\n",
    "    right_pattern = [z_binary[i] for i in RIGHT_LINE]\n",
    "    left_consistent = all(v == left_pattern[0] for v in left_pattern)\n",
    "    right_consistent = all(v == right_pattern[0] for v in right_pattern)\n",
    "    is_side_pose = (left_consistent and right_consistent and (left_pattern[0] != right_pattern[0]))\n",
    "    \n",
    "    use_z = not is_side_pose  # 정면일 때만 z 반영, 측면은 False\n",
    "\n",
    "    angle_text = \"\"\n",
    "    if not is_full_body(landmarks):\n",
    "        pred_class = 'etc'\n",
    "        posture_ok = None\n",
    "    else:\n",
    "        data = Data(\n",
    "            x=torch.tensor([[lm.x, lm.y, lm.z] for lm in landmarks], dtype=torch.float),\n",
    "            edge_index=edge_index\n",
    "        )\n",
    "        data.batch = torch.zeros(data.x.size(0), dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            probs = F.softmax(out, dim=1).cpu().numpy()[0]\n",
    "            pred_index = probs.argmax()\n",
    "            pred_class = label_encoder.inverse_transform([pred_index])[0]\n",
    "\n",
    "        if pred_class == 'etc':\n",
    "            posture_ok = None  # etc는 평가 제외\n",
    "        else:\n",
    "            neck_angle = calculate_neck_tilt_weighted(landmarks, z_binary, use_z)\n",
    "            waist_angle = calculate_waist_angle_weighted(landmarks, z_binary, use_z)\n",
    "\n",
    "            # print(f\"[DEBUG] pred_class: {pred_class}, neck_angle: {neck_angle}, waist_angle: {waist_angle}\")\n",
    "\n",
    "            posture_ok = check_posture_angles(pred_class, neck_angle, waist_angle, 'side' if is_side_pose else 'front')\n",
    "\n",
    "            # print(f\"[DEBUG] posture_ok: {posture_ok}\")\n",
    "\n",
    "    h, w, _ = image.shape\n",
    "\n",
    "    if pred_class == 'etc':\n",
    "        posture_ok = None\n",
    "        color = (0, 255, 255)\n",
    "        status_text = \"Class: etc (Not Evaluated)\"\n",
    "        pose_direction_text = \"\"\n",
    "        # angle_mode_text = \"\"\n",
    "    else:\n",
    "        if posture_ok == True or posture_ok is True or bool(posture_ok) == True:\n",
    "            color = (0, 255, 0)\n",
    "            status_desc = 'Good'\n",
    "        else:\n",
    "            color = (0, 0, 255)\n",
    "            status_desc = 'Bad'\n",
    "        status_text = f\"Class: {pred_class} ({status_desc})\"\n",
    "        angle_text = f\"Neck: {neck_angle:.1f}°, Waist: {waist_angle:.1f}°\"\n",
    "        pose_direction_text = f\"Pose: {'Side' if is_side_pose else 'Front'}\"\n",
    "        # angle_mode_text = f\"Angle Mode: {'2D' if not use_z else '3D'}\"\n",
    "\n",
    "    draw_skeleton(image, landmarks, edge_index)\n",
    "    # print(f\"[DEBUG] status_text: {status_text}, color: {color}, posture_ok: {posture_ok}\")\n",
    "\n",
    "    cv2.putText(image, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "    if angle_text:\n",
    "        cv2.putText(image, angle_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "    cv2.putText(image, pose_direction_text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    # cv2.putText(image, angle_mode_text, (10, 115), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    return posture_ok, image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93da461",
   "metadata": {},
   "source": [
    "# 웹캠 실시간 처리 및 녹화 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b69d51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1754194150.680810  273490 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1754194150.719537  273497 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# --- 웹캠 실시간 처리 및 녹화 저장 함수 ---\n",
    "def run_on_webcam_save(model, label_encoder, output_path=\"webcam_result.mp4\"):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"웹캠을 열 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps == 0 or fps != fps:  # fps가 0이나 NaN인 경우 기본값 지정\n",
    "        fps = 30.0\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    print(\"웹캠 자세 인식 및 녹화 시작. 'q' 키를 눌러 종료하세요.\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"프레임을 읽을 수 없습니다.\")\n",
    "            break\n",
    "\n",
    "        posture_ok, annotated = process_frame(frame, model, label_encoder, edge_index_vis, mp_pose)\n",
    "        if annotated is not None:\n",
    "            out.write(annotated)\n",
    "            cv2.imshow(\"Webcam Posture Detection\", annotated)\n",
    "        else:\n",
    "            out.write(frame)\n",
    "            cv2.imshow(\"Webcam Posture Detection\", frame)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"녹화가 완료되었습니다: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "19064b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "웹캠 자세 인식 및 녹화 시작. 'q' 키를 눌러 종료하세요.\n",
      "녹화가 완료되었습니다: webcam_result.mp4\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_on_webcam_save(model, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68696acf",
   "metadata": {},
   "source": [
    "# 맥 화면 실시간 자세 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae03689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mss import mss\n",
    "import time\n",
    "\n",
    "# QuickTime 영상 위치와 크기 (수동 입력 예시)\n",
    "monitor = {\"top\": 20, \"left\": 0, \"width\": 800, \"height\": 500}  # 필요 시 수정\n",
    "\n",
    "def run_on_screen_capture(model, label_encoder):\n",
    "    sct = mss()\n",
    "    print(\"QuickTime 화면에서 실시간 자세 분석 중... (ESC or Ctrl+C 로 종료)\")\n",
    "\n",
    "    while True:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # 화면 캡처\n",
    "        sct_img = sct.grab(monitor)\n",
    "        frame = np.array(sct_img)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "        # 자세 분석\n",
    "        _, annotated = process_frame(frame, model, label_encoder)\n",
    "\n",
    "        # 결과 출력\n",
    "        cv2.imshow(\"iPhone Camera Posture Detection\", annotated)\n",
    "\n",
    "        # ESC 누르면 종료\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "\n",
    "        # FPS 제한\n",
    "        elapsed = time.time() - start_time\n",
    "        time.sleep(max(1.0 / 30 - elapsed, 0))  # 30fps로 제한\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_on_screen_capture(model, label_encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_m3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
